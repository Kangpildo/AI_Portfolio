# 실습 Overview: LangugeModel_BERT

## 1. 실습 목적
- BERT(Bidirectional Encoder Representations from Transformers) 기반 한국어 자연어 처리(NLP) 모델의 이해와 실습.
- 사전학습된 언어모델의 임베딩 생성, 문장 간 의미 유사도 분석, 분류/추론 활용 방법 체험.

## 2. 주요 실습 내용
- **BERT 모델 이해**
  - Transformer 기반 구조 및 사전학습 개념 소개
  - Pre-trained 모델과 Fine-tuning의 차이 설명
- **Transformers 라이브러리 실습**
  - Hugging Face Transformers 및 Tokenizer 사용법 실습
- **임베딩 추출**
  - 입력 문장에 대한 BERT 임베딩 벡터 생성
  - 임베딩 벡터의 활용 예시(유사도 측정 등)
- **문장 유사도 계산**
  - 두 문장 임베딩 간 Cosine Similarity 등 거리 기반 유사도 측정 실습
- **문장 분류/추론**
  - Pre-trained BERT로 간단한 분류/추론 예제 실습
  - 추가 fine-tuning 없이 inference로 적용

## 3. 결과 및 기대 효과
- 자연어 문장 임베딩의 개념과 실용성 체험
- 문장 간 유사도, 분류, 추론 등 실무 활용 기초 습득
- Hugging Face 기반의 NLP 실험 기본기 확보

## 4. 참고 및 확장
- 사전학습 언어모델의 다른 활용(파인튜닝, QA 등) 실습으로 확장 가능
- 다양한 한글/다국어 pre-trained 모델 실험 가능

