{"cells":[{"cell_type":"code","source":["!pip install Korpora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"5ahqXz9rdLUQ","executionInfo":{"status":"ok","timestamp":1744888465220,"user_tz":-540,"elapsed":2738,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"026dd7c3-6d7a-4dc5-c17c-af17fe363da2"},"id":"5ahqXz9rdLUQ","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Korpora\n","  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n","Collecting dataclasses>=0.6 (from Korpora)\n","  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n","Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.1.31)\n","Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Installing collected packages: dataclasses, Korpora\n","Successfully installed Korpora-0.2.0 dataclasses-0.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]},"id":"e770e9de3c8c414c917cba18b71884ad"}},"metadata":{}}]},{"cell_type":"code","execution_count":13,"id":"1f8e1c7d","metadata":{"id":"1f8e1c7d","executionInfo":{"status":"ok","timestamp":1744889431361,"user_tz":-540,"elapsed":2,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}}},"outputs":[],"source":["from torch import nn\n","\n","\n","class SentenceClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        n_vocab,\n","        hidden_dim,\n","        embedding_dim,\n","        n_layers,\n","        dropout=0.5,\n","        bidirectional=True,\n","        model_type=\"lstm\"\n","    ):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=embedding_dim,\n","            padding_idx=0\n","        )\n","        if model_type == \"rnn\":\n","            self.model = nn.RNN(\n","                input_size=embedding_dim,\n","                hidden_size=hidden_dim,\n","                num_layers=n_layers,\n","                bidirectional=bidirectional,\n","                dropout=dropout,\n","                batch_first=True,\n","            )\n","        elif model_type == \"lstm\":\n","            self.model = nn.LSTM(\n","            input_size=embedding_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=n_layers,\n","            bidirectional=bidirectional,\n","            dropout=dropout,\n","            batch_first=True,\n","            )\n","\n","        if bidirectional:\n","            self.classifier = nn.Linear(hidden_dim * 2, 1)\n","        else:\n","            self.classifier = nn.Linear(hidden_dim, 1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, inputs):\n","        embeddings = self.embedding(inputs)\n","        output, _ = self.model(embeddings)\n","        last_output = output[:, -1, :]\n","        last_output = self.dropout(last_output)\n","        logits = self.classifier(last_output)\n","        return logits"]},{"cell_type":"code","execution_count":1,"id":"c5f61d2a-a53b-4e37-b118-927bb59d346b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5f61d2a-a53b-4e37-b118-927bb59d346b","executionInfo":{"status":"ok","timestamp":1744888475227,"user_tz":-540,"elapsed":1853,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"22a0a00a-d19d-4b47-8b5e-9498e9254074"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : e9t@github\n","    Repository : https://github.com/e9t/nsmc\n","    References : www.lucypark.kr/docs/2015-pyconkr/#39\n","\n","    Naver sentiment movie corpus v1.0\n","    This is a movie review dataset in the Korean language.\n","    Reviews were scraped from Naver Movies.\n","\n","    The dataset construction is based on the method noted in\n","    [Large movie review dataset][^1] from Maas et al., 2011.\n","\n","    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n","\n","    # License\n","    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n","    Details in https://creativecommons.org/publicdomain/zero/1.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nsmc] download ratings_train.txt: 14.6MB [00:00, 96.3MB/s]                            \n","[nsmc] download ratings_test.txt: 4.90MB [00:00, 38.2MB/s]                           \n"]}],"source":["import pandas as pd\n","from Korpora import Korpora\n","\n","\n","corpus = Korpora.load(\"nsmc\")\n","corpus_df = pd.DataFrame(corpus.test)"]},{"cell_type":"code","execution_count":2,"id":"0a11418e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a11418e","executionInfo":{"status":"ok","timestamp":1744888481484,"user_tz":-540,"elapsed":46,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"a69f9697-0a42-45cc-8438-54cfe42cd14e"},"outputs":[{"output_type":"stream","name":"stdout","text":["|       | text                                                                                     |   label |\n","|------:|:-----------------------------------------------------------------------------------------|--------:|\n","| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n","|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n","|   199 | 신날 것 없는 애니.                                                                       |       0 |\n","| 12447 | 잔잔 격동                                                                                |       1 |\n","| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n","Training Data Size : 45000\n","Testing Data Size : 5000\n"]}],"source":["train = corpus_df.sample(frac=0.9, random_state=42)\n","test = corpus_df.drop(train.index)\n","\n","print(train.head(5).to_markdown())\n","print(\"Training Data Size :\", len(train))\n","print(\"Testing Data Size :\", len(test))"]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHmgqjqPd-GP","executionInfo":{"status":"ok","timestamp":1744888662068,"user_tz":-540,"elapsed":3045,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"995c08cc-17dc-41d4-a62b-f139f4810c63"},"id":"fHmgqjqPd-GP","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"]}]},{"cell_type":"code","execution_count":5,"id":"71519fa4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71519fa4","executionInfo":{"status":"ok","timestamp":1744888835472,"user_tz":-540,"elapsed":172073,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"7f0d6328-6762-4e00-e22d-9f9be1afd110"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n","5002\n"]}],"source":["from konlpy.tag import Okt\n","from collections import Counter\n","\n","\n","def build_vocab(corpus, n_vocab, special_tokens):\n","    counter = Counter()\n","    for tokens in corpus:\n","        counter.update(tokens)\n","    vocab = special_tokens\n","    for token, count in counter.most_common(n_vocab):\n","        vocab.append(token)\n","    return vocab\n","\n","\n","tokenizer = Okt()\n","train_tokens = [tokenizer.morphs(review) for review in train.text]\n","test_tokens = [tokenizer.morphs(review) for review in test.text]\n","\n","vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n","token_to_id = {token: idx for idx, token in enumerate(vocab)}\n","id_to_token = {idx: token for idx, token in enumerate(vocab)}\n","\n","print(vocab[:10])\n","print(len(vocab))"]},{"cell_type":"code","execution_count":6,"id":"f0a1c0ab-dd85-4f97-b013-c1e180141f83","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0a1c0ab-dd85-4f97-b013-c1e180141f83","executionInfo":{"status":"ok","timestamp":1744888859667,"user_tz":-540,"elapsed":333,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"1780881e-5b84-4f22-c2e9-42a5b81b59cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n"," 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n","    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n"]}],"source":["import numpy as np\n","\n","\n","def pad_sequences(sequences, max_length, pad_value):\n","    result = list()\n","    for sequence in sequences:\n","        sequence = sequence[:max_length]\n","        pad_length = max_length - len(sequence)\n","        padded_sequence = sequence + [pad_value] * pad_length\n","        result.append(padded_sequence)\n","    return np.asarray(result)\n","\n","\n","unk_id = token_to_id[\"<unk>\"]\n","train_ids = [\n","    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n","]\n","test_ids = [\n","    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n","]\n","\n","\n","max_length = 32\n","pad_id = token_to_id[\"<pad>\"]\n","train_ids = pad_sequences(train_ids, max_length, pad_id)\n","test_ids = pad_sequences(test_ids, max_length, pad_id)\n","\n","print(train_ids[0])\n","print(test_ids[0])"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","\n","train_ids = torch.tensor(train_ids)\n","test_ids = torch.tensor(test_ids)\n","\n","train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n","test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n","\n","train_dataset = TensorDataset(train_ids, train_labels)\n","test_dataset = TensorDataset(test_ids, test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkduiuShf_hK","executionInfo":{"status":"ok","timestamp":1744889205490,"user_tz":-540,"elapsed":12,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"cd234de5-c288-41e4-ccd8-799470b39eb2"},"id":"zkduiuShf_hK","execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-981b453c1bb2>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_ids = torch.tensor(train_ids)\n","<ipython-input-10-981b453c1bb2>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_ids = torch.tensor(test_ids)\n"]}]},{"cell_type":"code","execution_count":11,"id":"866ad18f-31e7-4480-8460-d0b3e0c8ad41","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"866ad18f-31e7-4480-8460-d0b3e0c8ad41","executionInfo":{"status":"ok","timestamp":1744889209009,"user_tz":-540,"elapsed":7,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"1f6600d2-fb77-44e2-fdcc-8c214708b267"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-981b453c1bb2>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_ids = torch.tensor(train_ids)\n","<ipython-input-11-981b453c1bb2>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_ids = torch.tensor(test_ids)\n"]}],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","\n","train_ids = torch.tensor(train_ids)\n","test_ids = torch.tensor(test_ids)\n","\n","train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n","test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n","\n","train_dataset = TensorDataset(train_ids, train_labels)\n","test_dataset = TensorDataset(test_ids, test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":14,"id":"3994b8de-7656-42b9-b191-9bceaddd9627","metadata":{"id":"3994b8de-7656-42b9-b191-9bceaddd9627","executionInfo":{"status":"ok","timestamp":1744889443706,"user_tz":-540,"elapsed":6123,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}}},"outputs":[],"source":["from torch import optim\n","\n","\n","n_vocab = len(token_to_id)\n","hidden_dim = 64\n","embedding_dim = 128\n","n_layers = 2\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","classifier = SentenceClassifier(\n","    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",").to(device)\n","criterion = nn.BCEWithLogitsLoss().to(device)\n","optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":15,"id":"dc9227be-a546-4157-b472-c6d66d6897ec","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dc9227be-a546-4157-b472-c6d66d6897ec","executionInfo":{"status":"ok","timestamp":1744889654398,"user_tz":-540,"elapsed":57704,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"f64646c0-ee4c-497f-e681-61d88f4a597c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Loss 0 : 0.705612063407898\n","Train Loss 500 : 0.6936189903470571\n","Train Loss 1000 : 0.6885155416154242\n","Train Loss 1500 : 0.6775772050767641\n","Train Loss 2000 : 0.6714472700243649\n","Train Loss 2500 : 0.6662682001827145\n","Val Loss : 0.6254808262895091, Val Accuracy : 0.6758\n","Train Loss 0 : 0.5324637293815613\n","Train Loss 500 : 0.6093080251992582\n","Train Loss 1000 : 0.589187568003362\n","Train Loss 1500 : 0.5743958220570823\n","Train Loss 2000 : 0.5614153731843342\n","Train Loss 2500 : 0.5484936777137366\n","Val Loss : 0.4761131822396391, Val Accuracy : 0.7766\n","Train Loss 0 : 0.4409734010696411\n","Train Loss 500 : 0.4346628499602129\n","Train Loss 1000 : 0.4375459529853963\n","Train Loss 1500 : 0.43448479417500535\n","Train Loss 2000 : 0.43017174478905135\n","Train Loss 2500 : 0.42685251363643595\n","Val Loss : 0.4122879017180147, Val Accuracy : 0.8098\n","Train Loss 0 : 0.22991026937961578\n","Train Loss 500 : 0.3636095060886975\n","Train Loss 1000 : 0.3630452565141729\n","Train Loss 1500 : 0.36215404585391026\n","Train Loss 2000 : 0.3633760797853085\n","Train Loss 2500 : 0.3632134874059004\n","Val Loss : 0.3930110856176565, Val Accuracy : 0.8162\n","Train Loss 0 : 0.20773792266845703\n","Train Loss 500 : 0.3077718038446532\n","Train Loss 1000 : 0.3210759866539832\n","Train Loss 1500 : 0.32449030615811264\n","Train Loss 2000 : 0.3257486314411791\n","Train Loss 2500 : 0.3280599725316139\n","Val Loss : 0.3840548847906125, Val Accuracy : 0.8246\n"]}],"source":["def train(model, datasets, criterion, optimizer, device, interval):\n","    model.train()\n","    losses = list()\n","\n","    for step, (input_ids, labels) in enumerate(datasets):\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        logits = model(input_ids)\n","        loss = criterion(logits, labels)\n","        losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        if step % interval == 0:\n","            print(f\"Train Loss {step} : {np.mean(losses)}\")\n","\n","\n","def test(model, datasets, criterion, device):\n","    model.eval()\n","    losses = list()\n","    corrects = list()\n","\n","    for step, (input_ids, labels) in enumerate(datasets):\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        logits = model(input_ids)\n","        loss = criterion(logits, labels)\n","        losses.append(loss.item())\n","        yhat = torch.sigmoid(logits)>.5\n","        corrects.extend(\n","            torch.eq(yhat, labels).cpu().tolist()\n","        )\n","\n","    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n","\n","\n","epochs = 5\n","interval = 500\n","\n","for epoch in range(epochs):\n","    train(classifier, train_loader, criterion, optimizer, device, interval)\n","    test(classifier, test_loader, criterion, device)"]},{"cell_type":"code","execution_count":16,"id":"ee7ee60e-31ca-4722-85d1-00c1d48bc66a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee7ee60e-31ca-4722-85d1-00c1d48bc66a","executionInfo":{"status":"ok","timestamp":1744890128715,"user_tz":-540,"elapsed":41,"user":{"displayName":"PD Kang","userId":"02147936860241431184"}},"outputId":"4ca11933-6d49-41fa-cf70-a620849d9525"},"outputs":[{"output_type":"stream","name":"stdout","text":["보고싶다 [-1.0139822  -1.1990539  -2.4558291  -2.03313     0.4096605   1.7828662\n"," -0.30421942  0.07874438  1.5165144   0.23093839 -0.19860017 -1.3084415\n","  0.1354171   0.1064291   0.65044177  0.8382671  -1.2893771   0.9806419\n","  0.52236027  1.4098979  -0.48922485  1.8729188  -1.9653047   1.1720761\n"," -0.95867753  0.82752717 -1.4341388  -0.16536863 -2.1833937   0.6848712\n"," -0.08825599 -0.30593687 -0.80031997  2.34716    -0.5441059   0.0451383\n"," -0.66654575 -0.66944396  0.6466267   0.36958754 -0.4013479  -0.7062987\n","  1.2153533  -1.2373401   0.22397304 -0.20314673  0.6358524   1.2344646\n","  0.00763183  0.5867267  -0.6858376  -1.5501201  -1.3929126  -1.1201631\n"," -1.0798345   1.0274663  -0.29243505 -0.30465937  0.49547473  1.6545683\n","  1.2306327  -0.87845266 -0.6877592   0.72325844 -0.96803486  0.8214415\n"," -1.2143018  -1.1676109  -0.2543909   0.32786602  0.13063793 -0.09688384\n"," -1.4836155   0.02509729 -1.001553   -0.9726731   0.5892112   1.4400866\n","  0.5958896   1.0901295  -0.03057939  0.604845    2.7109833  -0.8455934\n"," -0.29608178 -1.2193235   0.07205349 -1.4722992   0.38714027 -0.5033349\n"," -0.82426137  2.0744538   0.66778374  1.8597604   1.4395093  -0.93319917\n","  0.50467217 -1.2532749  -0.32681075  1.0361068  -0.911964    0.00891047\n","  0.29446453  0.37332153 -1.8115615   1.1032122  -0.5040942   1.7125837\n","  1.9098643   1.094538   -0.85121936 -0.7915501   0.31606495  0.3143611\n","  0.10055203  0.48017627 -0.21637216 -0.70436573 -0.61867505 -0.55524564\n"," -2.2569983  -1.018443    0.4825966   0.5642365   0.07966395  0.89937866\n","  1.2373333  -1.3718331 ]\n"]}],"source":["token_to_embedding = dict()\n","embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n","\n","for word, emb in zip(vocab, embedding_matrix):\n","    token_to_embedding[word] = emb\n","\n","token = vocab[1000]\n","print(token, token_to_embedding[token])"]},{"cell_type":"code","source":[],"metadata":{"id":"tpVu_7z4jmf7"},"id":"tpVu_7z4jmf7","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1t1iYjLxS1JREsWKJNDMRTZO492XxhIg9","timestamp":1744810295265}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}